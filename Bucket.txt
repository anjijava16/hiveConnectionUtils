Base Raw table ===>Intetermideate table ==>stg/Delta tables 


BroadCast Hash Join: Small Table
Shuffle Hash join:Shuffle both tables,Hash smaller one ,stream the bigger one 
Sort merge join: Shuffle both tables,sort both tables,buffer one stream the bigger one
what is cost of running query(read)/CPU 
what is latency of the query .


When to use bucketing ?

1) Tables used frquenctly in join with Same Key
Ex   student -->student_id
     Employee --> employee_id
	 Users  --->user_id
	 
	 Those are the dimession tables;
	 
2) Loading daily cumulative tables;
    Both base and delta tables could be bucketed on common column	 
	 
3)Indexing Capability 	 
	

Spark Bucketing
	 
	 
	 df.write
	   .bucketBy(numBucket,"col1",....)
	   .sortBy("col1",....)
	   .saveAsTabke("Bucked_table")
	   
	   val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc);
  sqlContext.sql("use iwinnerdb");
 sqlContext.sql("desc formatted bucketed_user").collect.foreach(println)
 
 SET spark.sql.source.bucketing.enabled=true 
 
Spark-15453: https://issues.apache.org/jira/browse/SPARK-15453

https://github.com/apache/spark/pull/14864
Hive Hasing Function: Spark-17495
Enable Hive Bucketing tables Spark-17729

Hive Bucketing information for panner 17654

Join the predicate should not container filter clauses : spark-17698

https://blog.deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/

SortMerge Join ===> Sort ==> TableScan 
 select * from tableA join tableB on tabA.i=tabB.i and tabA.j=tabB.j
	   
http://www.learn4master.com/big-data/hive/hive-partitioning-vs-bucketing

http://grokbase.com/t/hive/user/162pdh84rs/spark-sql-is-not-returning-records-for-hive-bucketed-tables-on-hdp
https://blog.deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/

CREATE TEMPORARY TABLE temp_user(
firstname VARCHAR(64),
lastname  VARCHAR(64),
address   STRING,
country   VARCHAR(64),
city      VARCHAR(64),
state     VARCHAR(64),
post      STRING,
phone1    VARCHAR(64),
phone2    STRING,
email     STRING,
web       STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/anjaiahspr/UserRecords.txt' INTO TABLE temp_user;


CREATE TEMPORARY TABLE temp_user_bucket(
firstname VARCHAR(64),
lastname  VARCHAR(64),
address   STRING,
country   VARCHAR(64),
city      VARCHAR(64),
state     VARCHAR(64),
post      STRING,
phone1    VARCHAR(64),
phone2    STRING,
email     STRING,
web       STRING
)
CLUSTERED BY (state) 
INTO 32000 BUCKETS STORED AS SEQUENCEFILE;

ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
CLUSTERED BY (state) 
STORED AS TEXTFILE;

CREATE TABLE bucketed_user(
firstname VARCHAR(64),
lastname  VARCHAR(64),
address   STRING,city VARCHAR(64),
state     VARCHAR(64),
post      STRING,
phone1    VARCHAR(64),
phone2    STRING,
email     STRING,
web       STRING
)
PARTITIONED BY (country VARCHAR(64))
#CLUSTERED BY (state) SORTED BY (city.DESC) INTO 4 BUCKETS
DISTRIBUTED BY (state) SORTED BY (city) INTO 4 BUCKETS
STORED AS SEQUENCEFILE;


INSERT OVERWRITE TABLE bucketed_user PARTITION (country)
SELECT  firstname ,
lastname  ,
address   ,
city      ,
state     ,
post      ,
phone1    ,
phone2    ,
email     ,
web       ,
country   
FROM temp_user;



SELECT firstname, country, state, city FROM bucketed_user  TABLESAMPLE(BUCKET 32 OUT OF 32 ON state);



CREATE TABLE bucketed_user_no_sort(
firstname VARCHAR(64),
lastname  VARCHAR(64),
address   STRING,city VARCHAR(64),
state     VARCHAR(64),
post      STRING,
phone1    VARCHAR(64),
phone2    STRING,
email     STRING,
web       STRING
)
PARTITIONED BY (country VARCHAR(64))
CLUSTERED BY (state)INTO 4 BUCKETS
STORED AS TextFILE;

INSERT OVERWRITE TABLE bucketed_user_no_sort PARTITION (country)
SELECT  firstname ,
lastname  ,
address   ,
city      ,
state     ,
post      ,
phone1    ,
phone2    ,
email     ,
web       ,
country   
FROM temp_user;

SELECT firstname, country, state, city FROM bucketed_user_no_sort  TABLESAMPLE(BUCKET 32 OUT OF 32 ON state);


SELECT firstname, country, state, city FROM bucketed_user TABLESAMPLE(1 PERCENT);
SELECT firstname, country, state, city FROM bucketed_user  TABLESAMPLE(BUCKET 32 OUT OF 32 ON state);
 Select * from employee tablesample(10 rows)

 
Sysnatx: Table Sample (bucket x out of y [On ColName])
 (OR): Tablesample(n Rows)
 
 Select * from employee tablesample(10 rows)
==========================================================================================================
Sorted BY
Order BY
DisAdv : 
   Output will e single output file which is fully ordered)

DISTRIBUTE BY
CLUSTER BY

Sorted By:

Hive uses SORT BY to sort the rows based on the given columns per reducer. If there are more than one reducer, then the output per reducer will be sorted, but the order of total output is not guaranteed to be sorted.

Note: Hive uses the columns in SORT BY to sort the rows before feeding the rows to a reducer. The sort order will be dependent on the column types. If the column is of numeric type, then the sort order is also in numeric order. If the column is of string type, then the sort order will be lexicographical order.

set mapred.reduce.tasks=2;

We can see that the rows in Red are sorted and rows in Blue are sorted among themselves. But the overall sorting order is not maintained. Both these set of rows were processed by different reducers. Sort by is used to sort the data in each reducer according to the user specified order.

Note:: 
Sort by is used to sort the data in each reducer according to the user specified order.

==========================================================================================================
ORDER BY
Order by guarantees the total ordering of the output. Even if there are multiple reducers, the overall order of the output is maintained.
We can see that the overall order of sorting is maintained in the result by using Order By.

This is similar to ORDER BY in SQL Language.

In Hive, ORDER BY guarantees total ordering of data, but for that it has to be passed on to a single reducer, which is normally performance intensive and therefore in strict mode, hive makes it compulsory to use LIMIT with ORDER BY so that reducer doesn’t get overburdened.

Ordering : Total Ordered data.

Outcome : Single output i.e. fully ordered.

==========================================================================================================
DISTRIBUTE BY
Distribute By is used to distribute the rows to different reducers based on the value(s) of column(s). All rows with the same Distribute By columns will go to the same reducer. This is like partitioning in Map-Reduce, where all the records having same value of partition goes to the same reducer. Distribute By does not guarantee clustering the rows, based on the distributed By columns, in the reducers.

Note::
Distribute by with Sort by

If we club Distribute by with Sort by, then we can control the clustering of rows inside each reducer based on the value of some columns.

CLUSTER BY uses hash of clustering columns mod number of reducers to ensure rows with those same column values go to the same reducer - that's it, no stronger guarantees than that! See my answer with links to example and order preserving hashing etc. 

==========================================================================================================


CLUSTER BY
CLUSTER BY==Distribute by ++ Sort by’

http://myitlearnings.com/bucketing-in-hive/

http://www.learn4master.com/big-data/hive/hive-partitioning-vs-bucketing

http://grokbase.com/t/hive/user/162pdh84rs/spark-sql-is-not-returning-records-for-hive-bucketed-tables-on-hdp
https://blog.deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/

https://blog.deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/


======================

=>	Main purpose of buckets is sampling	
		       ---
		      |	5 |
		      |	4 |
		      |	10|
		      |	8 |
	 clustered by |	3 |
	 ------------ |	7 |
	| 	      |	5 |
	|	      |	9 |
	|	      |	6 |
	|	      |	2 |
	|	      |	1 |
        |  	       ---
	|		| distributed by
	|		|
	v		v
       ---	       ---	       ---	
      |	4 | sorted by |	5 |  order by |	1 |
      |	5 |  <--------|	4 |  -------> |	2 |
      |	8 |	      |	10|	      |	3 |
      |	10|	      |	8 |	      |	4 |
      |---|	      |---|	      |	5 |
      |	3 |	      |	3 |	      |	6 |
      |	6 |	      |	7 |	      |	7 |
      |	7 |	      |	9 |	      |	8 |
      |	9 |	      |	6 |	      |	9 |
      |---|	      |---|	      |	10|
      |	1 |	      |	2 |	       ---	
      |	2 |	      |	1 |
       ---	       ---						

=>	we can cluster by more than one columns

=>	By default bucketization is false. Enable hive.enforce.bucketing=true in hive-site.xml

=>	clustered by = sort by + distributed by

=>	sorted by is only during read(retrieval) not while write time

=>	

	1 % 4 = 1 
		
	2 % 4 = 2  
		   
	3 % 4 = 3  	 		part-0	=>	4,8
		   
	4 % 4 = 0  			part-1	=>	1,5,9
			  
	5 % 4 = 1	   		part-2	=>	2,6,10
			    	
	6 % 4 = 2	     		part-3	=>	3,7

	7 % 4 = 3

	8 % 4 = 0

	9 % 4 = 1

	10 % 4 = 2

=>		--------------------------------------------------------------------------
		Bucket column	N=2 then x	N=3 then x	N=4 then x	N=5 then x
		--------------------------------------------------------------------------
		1		1%2=1		1%3=1		1%4=1		1%5=1
		2		2%2=0		2%3=2		2%4=2		2%5=2
		3		3%2=1		3%3=0		3%4=3		3%5=3
		4		4%2=0		4%3=1		4%4=0		4%5=4		
		5		5%2=1		5%3=2		5%4=1		5%5=0
		6		6%2=0		6%3=0		6%4=2		6%5=1	
		7		7%2=1		7%3=1		7%4=3		7%5=2
		8		8%2=0		8%3=2		8%4=0		8%5=3
		9		9%2=1		9%3=0		9%4=1		9%5=4
		10		10%2=0		10%3=1		10%4=2		10%5=0

=>	bucket m out of n => if ( x == m - 1) then print ( bucket_col )

	bucket 1 out of 5 => if ( x == 0)	then print bucket_col	=>	5,10
	       2 out of 5 => if ( x == 1) 	then print bucket_col 	=>	1,6
	       3 out of 5 => if ( x == 2) 	then print bucket_col 	=>	2,7
	       4 out of 5 => if ( x == 3) 	then print bucket_col 	=>	3,8
	       5 out of 5 => if ( x == 4) 	then print bucket_col 	=>	4,9


	bucket 1 out of 4 => if ( x == 0)	then print bucket_col	=>	4,8
	       2 out of 4 => if ( x == 1) 	then print bucket_col 	=>	1,5,9
	       3 out of 4 => if ( x == 2) 	then print bucket_col 	=>	2,6,10
	       4 out of 4 => if ( x == 3) 	then print bucket_col 	=>	3,7

=>	rand() : when table is not bucketized then we dont have any bucketed columns and we want to apply bucketization,we use rand function 

=>	Difference between bucketization and partitions:

	In bucketization,we need to define no of buckets that is no of buckets is fixed which cannot be changed later. This is used for static data. If data is dynamic then go for partitioning because we dont define no of partition columns,we just define partitioning column.

=>	In partitions,no of partition columns are fixed.we cant change the partition columns later.we can use for static and dynamic data also.Static data(static partition)/dynamic data(dynamic partition)

=>	Hive		=>	MapReduce
	---------------------------------------
	Partition	=>	MultipleOutputs
	Bucket		=>	HashPartitioner

	Note:Limitations of 'HashPartitioner' is no of partitions are fixed.we can solve this problem using 'MultipleOutputs'


Join of 2 tables that are bucketed on the same columns.
which include the join columns. can be effciently implemented as a map side join

Cluster by clause to specify the columns to bucket on and the Number of buckets


Bucketing is a partitioning technique that can improve performance in certain data transformations by avoiding data shuffling and sorting. The general idea of bucketing is to partition, and optionally sort, the data based on a subset of columns while it is written out (a one-time cost), while making successive reads of the data more performant for downstream jobs if the SQL operators can make use of this property. Bucketing can enable faster joins (i.e. single stage sort merge join), the ability to short circuit in FILTER operation if the file is pre-sorted over the column in a filter predicate, and it supports quick data sampling. 
 
 In this session, you'll learn how bucketing is implemented in both Hive and Spark. In particular, Patil will describe the changes in the Catalyst optimizer that enable these optimizations in Spark for various bucketing scenarios. Facebook's performance tests have shown bucketing to improve Spark performance from 3-5x faster when the optimization is enabled. Many tables at Facebook are sorted and bucketed, and migrating these workloads to Spark have resulted in a 2-3x savings when compared to Hive. You'll also hear about real-world applications of bucketing, like loading of cumulative tables with daily delta, and the characteristics that can help identify suitable candidate jobs that can benefit from bucketing.
